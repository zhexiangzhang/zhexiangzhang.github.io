---
title: "TwinPilots: A New Computing Paradigm for GPU-CPU Parallel LLM Inference"
date: 2024-12-30
type: post
# draft: true
tags: ["LLM", "Inference System", "Heterogeneous"]
showTableOfContents: true
---

# TwinPilots: A New Computing Paradigm for GPU-CPU Parallel LLM Inference

> Systor 2024 \
> Chengye Yu, Song Jiang \
> Chinese University of Hong Kong, University of Texas at Arlington

## Background

When serving LLMs, GPU has insufficient memory and runs at a much slower speed due to constantly waiting for data to be loaded from the CPU memory via a slow PCIe bus.

## Solution

#### Key Idea
Break down each transformer layer into a set of constituent operations and estimate the operationsâ€™ CPU execution time and GPU execution time (including the dataâ€™s loading time into GPU) BASED on the history time. Then determine operations should be executed on the CPU or GPU.

#### Profiling
<img src="../../../../images/LLM/TwinPilots1.png" alt="" width="46%">

The data loading time does significantly surpass the GPU computing time!

<img src="../../../../images/LLM/TwinPilots2.png" alt="" width="51%">

#### Modeling
Model the operations placement problem as a load balancing problemï¼š
<br>
- given a set *ops* containing *ğ‘* operations and 2 devices: GPU and CPU
- the processing time of operation ğ‘– is ğ‘”ğ‘ğ‘¢_ğ‘– on GPU and ğ‘ğ‘ğ‘¢_ğ‘– on CPU. 
- ğ‘‘ğ‘’ğ‘£ğ‘–ğ‘ğ‘’_ğ‘– denote the computation device of operation ğ‘– (1 for CPU and 0 for GPU)
- total CPU processing time and GPU processing time of a certain plan ğ· = {ğ‘‘ğ‘’ğ‘£ğ‘–ğ‘ğ‘’_ğ‘– ; ğ‘–=1, . . .,ğ‘ }
   <br>ğ‘†_ğ‘ğ‘ğ‘¢ = SUM(ğ‘ğ‘ğ‘¢_ğ‘– Ã— ğ‘‘ğ‘’ğ‘£ğ‘–ğ‘ğ‘’_ğ‘–) and ğ‘†_ğ‘”ğ‘ğ‘¢ = SUM(ğ‘”ğ‘ğ‘¢_ğ‘– Ã— (1âˆ’ğ‘‘ğ‘’ğ‘£ğ‘–ğ‘ğ‘’_ğ‘–)

The makespan ğœ† of the plan is max(ğ‘†ğ‘ğ‘ğ‘¢, ğ‘†ğ‘”ğ‘ğ‘¢). The objective is to find a plan ğ· that minimizes the ğœ†
<br><br><img src="../../../../images/LLM/TwinPilots5.png" alt="" width="50%">

Note:the load balancing problem is NP-complete, but the small solution space is tolerable.

#### System Design
<img src="../../../../images/LLM/TwinPilots3.png" alt="" width="40%">
<br>TwinPilots comprises an online scheduler and a model executor. 

- online scheduler: collects historical execution info from statistics tracker and generates execution plans for each operation. <span style="color:#9d9e8d;">(Statistics records are initiated by running several predefined batch sizes and sequence lengths offline)</span>    
- model executor: performs data prefetching and operation computation based on the plan, utilizing both CPU and GPU resources in a pipelined manner.

#### Further optimization
TwinPilots also support future pipeline parallelism on multiple GPUs by partitions CPU cores into distinct core groups. Each GPU collaborates with one CPU core group to form a worker, which handles a portion of consecutive layers. LLM layers are evenly distributed across workers.
<img src="../../../../images/LLM/TwinPilots4.png" alt="" width="44%">






#### Details
NOTE: è®¡ç®—æ¯ä¸ªæ“ä½œå¹³å‡æ—¶é—´æ—¶è¦è€ƒè™‘batch_sizeå’Œlength(scoreæ“ä½œ)
<br><br><img src="../../../../images/LLM/TwinPilots6.png" alt="" width="38%">
